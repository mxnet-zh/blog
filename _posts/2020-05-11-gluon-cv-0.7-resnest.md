---
title: GluonCV 0.7：下一代主干网络ResNeSt
author: 张钟越 Amazon Research Engineer
---

自从2012年ImageNet比赛冠军AlexNet推出以来，预训练的图像分类神经网络通常会被用于处理其他下游任务如目标检测，语义分割，实例分割或姿态估计。因为这类网络由各种任务共享，我们一般称其为“主干网络”。 尽管最近有大量的新型主干网络涌现，在ImageNet的分类精度上也取得了长足进步，但大部分最新的下游任务的研究依然在使用ResNet。虽然ResNet发表于2015年，但仍能经久不衰主要是由于其模块化的设计和迁移学习时的易用性。针对这两个特性，GluonCV团队在本版本（GluonCV 0.7）中加入我们最新设计的主干网络ResNeSt，全面提升图像分类以及下游任务的性能。

![](img/gluon-cv-0.7-resnest.png){:width="320"}

ResNeSt保留了ResNet固有的模块化和迁移学习能力，同时提高了各种视觉任务的准确性。和之前最强的主干网络EfficientNet相比，我们发现ResNeSt可以在GPU上同时提高速度和准确性。例如，我们的ResNeSt-269达到稍高于EfficientNet-B7的精度，同时将推理延迟降低了约30％。另外，我们发现通过简单的改动,ResNeSt既可以在下游任务上取得比ResNet跟高的准确度。在对象检测和语义分割这两项任务中，把ResNet换成ResNeSt，无需调整任何超参数，我们便可以提升Faster R-CNN COCO mAP约4％，提升DeepLabV3 ADE20K mIoU约3％。

## [图像分类](https://gluon-cv.mxnet.io/model_zoo/classification.html#resnest)

主干网络通常在ImageNet数据集上进行预训练，而其权重一般会用于各种下游任务。 因此，图像分类任务的准确度对于其他高层计算机视觉任务至关重要。 我们在GluonCV 0.7中新增了四个不同复杂度的ResNeSt主干网络。我们同时还提供了用于训练的相应代码，帮您轻松复现结果。在以前的版本中，我们最好的分类主干网络是SENet-154，该模型在ImageNet数据集上的top-1准确性达到了81.26％。而我们新设计的主干网络除ResNeSt-50之外，都比SENet-154（以前版本中最精确的模型）更准确。以下是详细结果：

| Model           | Input Size | Top-1 Acc (%) | Avg Latency (ms) | Release | 
| --------------- | ---------- | ------------- | ---------------- | ------- |
| SENet-154       | 224x224    | 81.3          | 5.07             | v0.6    |
| EfficientNet-B2 | 260x260    | 80.3          | 2.18             |         |
| EfficientNet-B3 | 300x300    | 81.7 	       | 2.96             |         |
| EfficientNet-B4 | 380x380    | 83.0          | 6.70             |         |
| EfficientNet-B5 | 456x456    | 83.7          | 11.86            |         |
| EfficientNet-B6 | 528x528    | 84.1          | 18.55            |         |
| EfficientNet-B7 | 600x600    | 84.4          | 28.64            |         |
| ResNeSt50       | 224x224    | 81.1          | 1.78             | v0.7    |
| ResNeSt101      | 256x256    | 82.8          | 3.43             | v0.7    |
| ResNeSt200      | 320x320    | 83.9          | 9.49             | v0.7    |
| ResNeSt269      | 416x416    | **84.5**      | 19.50            | v0.7    |

下图为我们在单个V100 GPU上使用批大小16进行推理的速度测试。相较于EfficientNet，ResNeSt可以达到更高的准确性和更低的延迟。

![](img/gluon-cv-0.7-resnest_vs_efficientnet.png){:width="320"}

## [目标检测](https://gluon-cv.mxnet.io/model_zoo/detection.html#id65)

ResNeSt在图像分类上取得了不错的成绩，但是它在其他下游任务上效果如何呢？为了验证ResNeSt可以改善下游任务，我们将Faster R-CNN中的ResNet替换为ResNeSt，结果平均精度均值（mAP）瞬间提高了3％，效果十分显著。 在GluonCV 0.7中，我们还为Faster R-CNN添加了新的训练技巧，例如同步Batch Normalization，随机图像缩放增强和更深的目标框分支（4个卷积+ 1个全连阶层）。 通过这些改进，我们把ResNeSt-50的mAP增加到42.7，这比先前ResNet-101的结果还要高。细心的小伙伴可能会注意到我们在GluonCV提供的模型比在论文中报道的要高一些，这是因为我们使用26个epochs而论文中使用的是13个epochs。

| Model        | Backbone    | mAP      | Release |
| ------------ | ----------- | -------- | ------- |
| Faster R-CNN | ResNet-101  | 40.8     | v0.6    |
| Faster R-CNN | ResNeSt-50  | 42.7     | v0.7    |
| Faster R-CNN | ResNeSt-101 | **44.9** | v0.7    |

## [语义分割](https://gluon-cv.mxnet.io/model_zoo/segmentation.html#ade20k-dataset)

在本版本中，我们还提供了两个新的语义分割模型。我们通过替换DeepLabV3的ResNet主干成新的ResNeSt，在均交并比（mIoU）上提高了2.8%，在像素精度(Pixel Accuracy)上提高了1%，并在ADE20K验证集取得了SoTA结果。通过简单的替换主干网络，ResNeSt便轻松取得优于其他许多专门为语义分割设计的模型，例如ACNet，HRNet等，验证了ResNeSt是能够泛化化到不同的任务，而且和ResNet使用完全相同的超参数。

| Model     | Backbone    | Pixel Accuracy | mIoU     | Release |
| --------- | ----------- | -------------- | -------- | ------- |
| DeepLabV3 | ResNet-101  | 81.1           | 44.1     | v0.6    |
| DeepLabV3 | ResNeSt-50  | 81.2           | 45.1     | v0.7    |
| DeepLabV3 | ResNeSt-101 | 82.1           | **46.9** | v0.7    |

## 小结

GluonCV 0.7加入了最新的图像分类主干网络，显着提升各项视觉任务的准确性。使用GluonCV 0.7，您可以非常方便的在研究或部署中使用我们最新的ResNeSt。有关更多详细信息，您可以阅读我们的[论文](https://arxiv.org/abs/2004.08955)。

## 鸣谢

我们由衷感谢以下的贡献者：
@zhreshold, @adursun, @KuangHaofei, @bryanyzhu, @FrankYoungchen, @ElectronicElephant, @lgov, @astonzhang, @ruslo, @mjamroz, @LauLauThom, @karan6181, @turiphro, @chinakook, @zhanghang1989, @Jerryzcn

## 相关链接

- [GluonCV 网站](https://gluon-cv.mxnet.io/index.html)
- [GluonCV Github](https://github.com/dmlc/gluon-cv)
- [论坛讨论](https://discuss.gluon.ai/)

喜欢我们的工作并且希望支持更多的更新，欢迎点赞加星Fork！
